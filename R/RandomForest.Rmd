---
title: "Predicting Rs Partitioning Using Random Forest Modeling"
author: "Max Frissell"
date: "8/7/2020"
output: html_document
---

```{r Packages, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(dplyr)
library(tidyr)
library(magrittr)
library(ggplot2)
theme_set(theme_minimal())
library(here)
library(hexbin)
library(cowplot)
library(plyr)
library(raster)
library(randomForest)
library(caret)
library(e1071)
```

```{r SRDBSetup, include = FALSE}
## Selects the lines I want to use from the SRDB

# Read in entire SRDB and pull only desired columns, removing rows with missing values
read.csv(here::here("Data", "srdb-data.csv")) %>%
  dplyr::select(Site_ID, Latitude, Longitude, Leaf_habit, Rs_annual, RC_annual, Manipulation) %>%
  subset(!is.na(Site_ID) & !is.na(Latitude) & !is.na(Longitude) & !is.na(Leaf_habit) & !is.na(Rs_annual) & RC_annual > 0 & RC_annual < 1 & !is.na(RC_annual) & Manipulation == "None") -> srdb
```

```{r PullingFromWC2}
## This chunk pulls climate data from WorldClim2 and makes some graphs from the mean annual temperature and precipitation data

# Download worldclim data for precip and tmean if necessary, into w10/ folder
precip <- getData("worldclim", path = here::here(), var = "prec", res = 10, download = !file.exists("wc10/prec1.hdr"))
tmean <- getData("worldclim", path = here::here(), var = "tmean", res = 10, download = !file.exists("wc10/wc10/tmean1.hdr"))

# Pull out cosore dataset latitudes and longitudes
srdb %>%
  dplyr::select(Site_ID, Longitude, Latitude) -> srdb_coords

# MAP data that matches the srdb coordinates
raster::extract(precip, srdb_coords[2:3]) -> precip_coords
apply(precip_coords, 1, sum) -> MAP

# The same for MAT
raster::extract(tmean, srdb_coords[2:3]) -> tmean_vals
apply(tmean_vals, 1, mean) -> MAT

# Temp data is stored in degC * 10, so we need to divide to get back to degC
MAT <- MAT/10

# Add the worldclim MAT and MAP values to the srdb data and remove missing values
allData = cbind(srdb, MAT, MAP)
allData = allData[!is.na(MAT),]
```

``` {r ExtractingMyco}
## Extracts data on mycorrhizae for each SRDB point from global dataset from Soudzilovskaia et al. 2019
## Note: the .TIF files used here::here are not included in the repo and were downloaded from https://github.com/nasoudzilovskaia/Soudzilovskaia_NatureComm_MycoMaps/tree/master/Maps_Myco_veg_current on 7/14/20

# Get all of the global data for each type of myco (again, files are pre-downloaded and not in the GitHub repo)
am = raster(here::here("Data", "MycDistrAM_current.TIF"))
em = raster(here::here("Data", "MycDistrEM_current.TIF"))
er = raster(here::here("Data", "MycDistrER_current.TIF"))
nm = raster(here::here("Data", "MycDistrNM_current.TIF"))

# Extract the myco data for each coordinate pair in the SRDB database and add it to the rest of the data
AM_percent = raster::extract(am, allData[, c(3, 2)])
EM_percent = raster::extract(em, allData[, c(3, 2)])
ER_percent = raster::extract(er, allData[, c(3, 2)])
NM_percent = raster::extract(nm, allData[, c(3, 2)])
allData = cbind(allData, AM_percent, EM_percent, ER_percent, NM_percent)

# Throw out the entries where::here there::here isn't myco data
allData = allData[!is.na(allData$AM_percent),]
```

``` {r ExtractingBiomass}
## Extracts data on above and below ground biomass for each SRDB point from global dataset from Spawn et al. 2020
## Note: the .TIF files used are not included in the repo and were downloaded from https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1763 on 7/15/20

raster(here::here("Data", "aboveground_biomass_carbon_2010.tif")) %>%
  raster::extract(allData[, c(3, 2)]) -> BM_aboveground

raster(here::here("Data", "belowground_biomass_carbon_2010.tif")) %>%
  raster::extract(allData[, c(3, 2)]) -> BM_belowground

# Add these biomass values to the larger dataset
allData = cbind(allData, BM_aboveground, BM_belowground)

# Remove values where::here there::here are no biomass data
allData = allData[!is.na(allData$BM_aboveground) & !is.na(allData$BM_belowground),]
```

``` {r ExtractingNDep}
## Extracts N deposition data for each srdb point

# Pull N-dep values for every coordinate pair in the SRDB
raster(here::here("Data", "sdat_830_2_20200721_153826639.asc")) %>%
  raster::extract(allData[, c(3, 2)]) -> N_dep_1993

# Add this to the data pool and remove entries without N-dep data
allData <- cbind(allData, N_dep_1993)[!is.na(N_dep_1993),]
```

``` {r ExtractingIGBP}
## Extracts climate and vegetation data from IGBP Koppen MODIS

IGBP_Koppen_MODIS <- read.csv(here::here("Data", "IGBP_Koppen_MODIS.csv"))

# Regroup climate data into fewer categories for easier analysis
IGBP_Koppen_MODIS %>% 
  mutate(MiddleClimate = case_when(
    ClimateTypes %in% c("Af", "Am", "As", "Aw") ~ "A",
    ClimateTypes %in% c("BSh", "BSk", "BWh", "BWk") ~ "B",
    ClimateTypes %in% c("Cfa", "Cfb", "Cfc") ~ "Cf",
    ClimateTypes %in% c("Csa", "Csb", "Csc") ~ "Cs",
    ClimateTypes %in% c("Cwa", "Cwb", "Cwc") ~ "Cw",
    ClimateTypes %in% c("Dfa", "Dfb", "Dfc", "Dfd") ~ "Df",
    ClimateTypes %in% c("Dsa", "Dsb", "Dsc", "Dwa", "Dwb", "Dwc", "Dwd") ~ "Dsw",
    ClimateTypes %in% c("EF", "ET") ~ "E",
    TRUE ~ "Other")) -> IGBP_Koppen_MODIS

# Change Latitude and Longitude to the same 0.5*0.5 resolution as in the dataset
mutate(allData, Latitude2 = round(Latitude*2)/2+0.25, Longitude2 = round(Longitude*2)/2+0.25) -> allData

# Add data to the large dataset
left_join(allData, IGBP_Koppen_MODIS, by=c("Latitude2" = "Latitude", "Longitude2" = "Longitude")) -> allData

# Remove data I don't want anymore and NAs
allData = subset(allData, select = -c(Latitude2, Longitude2, IGBP, Ecosystem, ClimateTypes, barren_yn, warner_rs))
```

### Predicting RC Using a Default Random Forest Model

First, I will use the default settings in the randomForest package to make a random forest model to predict the root contribution to Rs (RC). 

``` {r RandomForest}
## Use random forest machine learning to create a model to predict Rs partitioning

# Set seed to a specific value, for reproducability
set.seed(191982)

# Add the absolute value of latitude to the dataset
absLat = abs(allData$Latitude)
allData = cbind(allData, absLat)

# Make sure there is no missing data
allData = allData[!is.na(allData$RC_annual) & !is.na(allData$absLat) &
!is.na(allData$Leaf_habit) & !is.na(allData$Rs_annual) &
!is.na(allData$MAT) & !is.na(allData$MAP) &
!is.na(allData$AM_percent) & !is.na(allData$EM_percent) & !is.na(allData$ER_percent) & !is.na(allData$NM_percent) &
!is.na(allData$BM_aboveground) & !is.na(allData$BM_belowground) &
!is.na(allData$N_dep_1993) &
!is.na(allData$IGBP_group) & !is.na(allData$Ecosystem2) & !is.na(allData$MiddleClimate),]

# Split data into a set to train the model and a set to test approx. 70/30 split
samp = sample(2, nrow(allData), replace = TRUE, prob = c(.7, .3))
train = allData[samp == 1,]
test = allData[samp == 2,]

# Make a random forest model
model = randomForest(RC_annual ~
                       absLat +
                       Leaf_habit + Rs_annual +
                       MAT + MAP +
                       AM_percent + EM_percent + ER_percent + NM_percent +
                       BM_aboveground + BM_belowground +
                       N_dep_1993 +
                       IGBP_group + Ecosystem2 + MiddleClimate,
                     data = train,
                     importance = TRUE, 
                     proximity = TRUE,
                     na.action = na.exclude)
```

This compares the predictions the model made for the data it was trained with to the actually RC values.

``` {r InvestigateModelWithTrain}
## Investigate how well the model works at predicting training data RC

# Use this model to predict RC values for the training data and put it all in one dataframe
prediction = predict(model, train)
trainWithP = cbind(train, prediction)

# Plot training data RC values and the predicted values from the model
ggplot(trainWithP, aes(y = RC_annual, x = prediction)) +
  lims(x = c(0, 1)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1, color = "red")

# Calculate the residuals and make a residuals plot
residuals = trainWithP$RC_annual - trainWithP$prediction
trainWithP = cbind(trainWithP, residuals)
ggplot(trainWithP, aes(x = prediction, y = residuals)) +
  lims(x = c(0, 1)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0, linetype = "dashed", size = 1, color = "red")

# Get the average error for the RC predictions
abs(residuals) %>%
  na.omit() %>%
  sum()/length(residuals) -> average
```

The average prediction for the training data has a residual of `r average`.

This compares the predictions for data the model wasn't trained with with the actual RC values.

``` {r InvestigateModelWithTest}
## Investigate how well the model works at predicting test data RC

# Use the model to predict RC for the data we set aside
prediction = predict(model, test)
testWithP = cbind(test, prediction)

# Plot test data RC values with their predicted values
ggplot(testWithP, aes(x = prediction, y = RC_annual)) +
  lims(x = c(0, 1)) +
  geom_point() +
  geom_smooth(method = lm) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1, color = "red")

# Calculate the residuals and make a residuals plot
residuals = testWithP$RC_annual - testWithP$prediction
testWithP = cbind(testWithP, residuals)
ggplot(testWithP, aes(x = prediction, y = residuals)) +
  lims(x = c(0, 1)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0, linetype = "dashed", size = 1, color = "red")

# Get the average error for the RC predictions
abs(residuals) %>%
  na.omit() %>%
  sum()/length(residuals) -> average
```

The average prediction for the testing data has a residual of `r average`

### Improving the model

Here is a graph of the error vs the number of trees in the model and a graph of how the error varies with different values of mtry.
mtry is a parameter for this random forest function that sets how many variables are tried when making each node of a regression tree.
The number of trees and this mtry value are both input parameters used to produce the random forest model.

``` {r ModelTuning}
## This Will investigate certain facets of the model and tweak how it was made to hopefully make more accurate predictions

# Plot how much adding more trees affected the accuracy
plot(model)

# Tune with different mtry values
tuneRF(subset(train, select = -c(RC_annual, Site_ID, Longitude, Latitude, Manipulation)), 
       train[, 6],
       stepFactor = 2,
       plot = TRUE,
       ntreetry = 200, # Based on the plot above, 200 trees seems reasonable
       trace = TRUE,
       improve = .01)
```

An mtry of 10 seem the best the most often in this test.
100 trees would probably be enough, but I will use 200 to be safe.

To make my predictions more consistent, I will make 20 separate models, using these values for the number of trees and mtry, and average the outputs.

``` {r RemakeModelAndPredict}
## Make 20 RF models with the better settings and average their predictions

# Make 20 RF models
for(i in 1:20) {
  model = randomForest(RC_annual ~
                       absLat +
                       Leaf_habit + Rs_annual +
                       MAT + MAP +
                       AM_percent + EM_percent + ER_percent + NM_percent +
                       BM_aboveground + BM_belowground +
                       N_dep_1993 +
                       IGBP_group + Ecosystem2 + MiddleClimate,
                     data = train,
                     ntree = 200,
                     mtry = 10,
                     importance = TRUE, 
                     proximity = TRUE,
                     na.action = na.exclude)
  
  # Predict values for the training and testing data using each model
  prediction = predict(model, train)
  prediction2 = predict(model, test)
  
  # Add predictions to a dataframe
  if(i == 1) {
    allPredict = prediction
    allPredict2 = prediction2
  } else {
    allPredict = cbind(allPredict, prediction)
    allPredict2 = cbind(allPredict2, prediction2)
  }
}

# Calculate the sum of all the predictions for each point
for(i in 1:nrow(allPredict)) {
  sum = 0
  
  for(j in 1:ncol(allPredict)) {
    sum = sum + allPredict[i, j]
  }
  
  # Average all the predictions
  trainAvg = sum/ncol(allPredict)
  
  # Put all the averages in a vector
  if(i == 1) {
    trainPs = c(trainAvg)
  } else {
    trainPs = append(trainPs, trainAvg)
  }
}

# Do the same for test data predictions
for(i in 1:nrow(allPredict2)) {
  sum = 0
  
  for(j in 1:ncol(allPredict2)) {
    sum = sum + allPredict2[i, j]
  }
  
  testAvg = sum/ncol(allPredict2)
  
  if(i == 1) {
    testPs = c(testAvg)
  } else {
    testPs = append(testPs, testAvg)
  }
}
```

Here are how the averaged model predictions compared to the true RC values in the data used to train the models.

``` {r PredictTrain}
## Investigate predictions for training data

# Put everything into one dataframe
predict2 = trainPs
trainWithP = cbind(trainWithP, predict2)

# Plot predictions vs true values
ggplot(trainWithP, aes(x = predict2, y = RC_annual)) +
  lims(x = c(0, 1)) +
  geom_point() +
  geom_smooth(method = lm) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1, color = "red") +
  labs(x = "RC Prediction")

# Calculate the residuals and make a residuals plot
residuals2 = trainWithP$RC_annual - trainWithP$predict2
trainWithP = cbind(trainWithP, residuals2)
ggplot(trainWithP, aes(x = predict2, y = residuals2)) +
  lims(x = c(0, 1)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0, linetype = "dashed", size = 1, color = "red")

# Get the average error for the RC predictions
abs(residuals2) %>%
  na.omit() %>%
  sum()/length(residuals2) -> average
```

The average prediction for the training data has a residual of `r average`.

Here are how the averaged model predictions compared to the true RC values in the data not used to train the models.

``` {r PredictTest}
## Investigate predictions for test data

# put everything into one dataframe
predict2 = testPs
testWithP = cbind(testWithP, predict2)

# Plot predictions against the true values
ggplot(testWithP, aes(x = predict2, y = RC_annual)) +
  lims(x = c(0, 1)) +
  geom_point() +
  geom_smooth(method = lm) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1, color = "red") +
  labs(x = "RC Prediction")

# Calculate the residuals and make a residuals plot
residuals2 = testWithP$RC_annual - testWithP$predict2
testWithP = cbind(testWithP, residuals2)
ggplot(testWithP, aes(x = predict2, y = residuals2)) +
  lims(x = c(0, 1)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0, linetype = "dashed", size = 1, color = "red")

# Get the average error for the RC predictions
abs(residuals2) %>%
  na.omit() %>%
  sum()/length(residuals2) -> average
```

The average prediction for the training data has a residual of `r average`.

These are some values for linear models for RC based on the predicted values.
The first is for the training data, the second for the data set aside at the start.

``` {r CorelationValues}
## See how much of the variation in RC_annual is explained by out prediction value

# For training data
trainCor = lm(RC_annual ~ predict2, data = trainWithP)
summary(trainCor)

#For testing data
testCor = lm(RC_annual ~ predict2, data = testWithP)
summary(testCor)
```

``` {r MapResults}
## Map the predictions for all SRDB points

# Put all results in one dataframe
everything = rbind(trainWithP, testWithP)

# Make a map
ggplot(everything, aes(x = Longitude, y = Latitude, color = predict2)) +
  geom_point() +
  lims(x = c(-180, 180), y = c(-90, 90)) +
  labs(color = "RC prediction") +
  scale_color_viridis_c()
```